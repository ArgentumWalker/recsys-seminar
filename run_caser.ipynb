{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'caser_pytorch'...\n",
      "remote: Enumerating objects: 7, done.\u001b[K\n",
      "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
      "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
      "remote: Total 84 (delta 2), reused 6 (delta 2), pack-reused 77\u001b[K\n",
      "Unpacking objects: 100% (84/84), done.\n"
     ]
    }
   ],
   "source": [
    "# clone my edit of caser\n",
    "!git clone https://github.com/maxkvant/caser_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append('caser_pytorch')\n",
    "\n",
    "from train_caser import Recommender\n",
    "import argparse\n",
    "from interactions import Interactions\n",
    "from evaluation import evaluate_hits_ndcg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare SASRec datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'SASRec' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/kang205/SASRec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on https://github.com/kang205/SASRec/blob/master/util.py\n",
    "\n",
    "def data_partition(fname):\n",
    "    usernum = 0\n",
    "    itemnum = 0\n",
    "    User = defaultdict(list)\n",
    "    user_train = {}\n",
    "    user_valid = {}\n",
    "    user_test = {}\n",
    "    # assume user/item index starting from 1\n",
    "    with open(fname, 'r') as f:\n",
    "        for line in f:\n",
    "            u, i = line.rstrip().split(' ')\n",
    "            u = int(u)\n",
    "            i = int(i)\n",
    "            usernum = max(u, usernum)\n",
    "            itemnum = max(i, itemnum)\n",
    "            User[u].append(i)\n",
    "\n",
    "        for user in User:\n",
    "            nfeedback = len(User[user])\n",
    "            if nfeedback < 3:\n",
    "                user_train[user] = User[user]\n",
    "                user_valid[user] = []\n",
    "                user_test[user] = []\n",
    "            else:\n",
    "                user_train[user] = User[user][:-2]\n",
    "                user_valid[user] = []\n",
    "                user_valid[user].append(User[user][-2])\n",
    "                user_test[user] = []\n",
    "                user_test[user].append(User[user][-1])\n",
    "    return [user_train, user_valid, user_test, usernum, itemnum]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to(path, train, test):\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    train_path = path + '/train.txt'\n",
    "    test_path  = path + '/test.txt'\n",
    "    \n",
    "    with open(train_path, 'w') as f:\n",
    "        for user, items in train.items():\n",
    "            for item in items:\n",
    "                print(f'{user} {item} 1', file=f)\n",
    "    with open(test_path, 'w') as f:\n",
    "        for user, items in test.items():\n",
    "            for item in items:\n",
    "                print(f'{user} {item} 1', file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "steam_train, _, steam_test, _, _ = data_partition('SASRec/data/Steam.txt')\n",
    "save_to('data/Steam', steam_train, steam_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "beauty_train, _, beauty_test, _, _ = data_partition('SASRec/data/Beauty.txt')\n",
    "save_to('data/Beauty', beauty_train, beauty_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_train, _, movies_test, _, _ = data_partition('SASRec/data/ml-1m.txt')\n",
    "save_to('data/ml-1m', movies_train, movies_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_train, _, video_test, _, _ = data_partition('SASRec/data/Video.txt')\n",
    "save_to('data/Video', video_train, video_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 57M\n",
      "4.4M -rw-rw-r-- 1 1001 1003 4.4M Dec 14 14:03 Beauty.txt\n",
      "4.0K -rw-rw-r-- 1 1001 1003 1.5K Dec 14 14:03 DataProcessing.py\n",
      " 41M -rw-rw-r-- 1 1001 1003  41M Dec 14 14:03 Steam.txt\n",
      "3.1M -rw-rw-r-- 1 1001 1003 3.1M Dec 14 14:03 Video.txt\n",
      "8.7M -rw-rw-r-- 1 1001 1003 8.7M Dec 14 14:03 ml-1m.txt\n"
     ]
    }
   ],
   "source": [
    "!ls -shn SASRec/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/Beauty/:\n",
      "total 4.5M\n",
      "676K -rw-rw-r-- 1 1001 1003 675K Dec 14 14:22 test.txt\n",
      "3.8M -rw-rw-r-- 1 1001 1003 3.8M Dec 14 14:22 train.txt\n",
      "\n",
      "data/Steam/:\n",
      "total 44M\n",
      "4.5M -rw-rw-r-- 1 1001 1003 4.5M Dec 14 14:20 test.txt\n",
      " 39M -rw-rw-r-- 1 1001 1003  39M Dec 14 14:20 train.txt\n",
      "\n",
      "data/Video/:\n",
      "total 3.2M\n",
      "408K -rw-rw-r-- 1 1001 1003 406K Dec 14 14:23 test.txt\n",
      "2.8M -rw-rw-r-- 1 1001 1003 2.8M Dec 14 14:23 train.txt\n",
      "\n",
      "data/ml-1m/:\n",
      "total 11M\n",
      "68K -rw-rw-r-- 1 1001 1003 66K Dec 14 14:22 test.txt\n",
      "11M -rw-rw-r-- 1 1001 1003 11M Dec 14 14:22 train.txt\n"
     ]
    }
   ],
   "source": [
    "!ls -shn data/*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, _, test, _, _ = data_partition('beer_reviews.txt')\n",
    "save_to('data/Beer', train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/Beauty/:\n",
      "total 4.5M\n",
      "676K -rw-rw-r-- 1 1001 1003 675K Dec 14 14:22 test.txt\n",
      "3.8M -rw-rw-r-- 1 1001 1003 3.8M Dec 14 14:22 train.txt\n",
      "\n",
      "data/Beer/:\n",
      "total 38M\n",
      "300K -rw-rw-r-- 1 1001 1003 298K Dec 14 20:21 test.txt\n",
      " 37M -rw-rw-r-- 1 1001 1003  37M Dec 14 20:21 train.txt\n",
      "\n",
      "data/Steam/:\n",
      "total 44M\n",
      "4.5M -rw-rw-r-- 1 1001 1003 4.5M Dec 14 14:20 test.txt\n",
      " 39M -rw-rw-r-- 1 1001 1003  39M Dec 14 14:20 train.txt\n",
      "\n",
      "data/Video/:\n",
      "total 3.2M\n",
      "408K -rw-rw-r-- 1 1001 1003 406K Dec 14 14:23 test.txt\n",
      "2.8M -rw-rw-r-- 1 1001 1003 2.8M Dec 14 14:23 train.txt\n",
      "\n",
      "data/ml-1m/:\n",
      "total 11M\n",
      "68K -rw-rw-r-- 1 1001 1003 66K Dec 14 14:22 test.txt\n",
      "11M -rw-rw-r-- 1 1001 1003 11M Dec 14 14:22 train.txt\n"
     ]
    }
   ],
   "source": [
    "!ls -shn data/*/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on https://github.com/graytowne/caser_pytorch/blob/master/train_caser.py\n",
    "\n",
    "def get_model(n_iter=20):\n",
    "    def str2bool(v):\n",
    "        return v.lower() in ('true')\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    # data arguments\n",
    "    parser.add_argument('--L', type=int, default=5)\n",
    "    parser.add_argument('--T', type=int, default=3)\n",
    "    # train arguments\n",
    "    parser.add_argument('--n_iter', type=int, default=50)\n",
    "    parser.add_argument('--seed', type=int, default=1234)\n",
    "    parser.add_argument('--batch_size', type=int, default=512)\n",
    "    parser.add_argument('--learning_rate', type=float, default=1e-3)\n",
    "    parser.add_argument('--l2', type=float, default=1e-6)\n",
    "    parser.add_argument('--neg_samples', type=int, default=3)\n",
    "    parser.add_argument('--use_cuda', type=str2bool, default=True)\n",
    "\n",
    "    config = parser.parse_args(\"\")\n",
    "\n",
    "    # model dependent arguments\n",
    "    model_parser = argparse.ArgumentParser()\n",
    "    model_parser.add_argument('--d', type=int, default=50)\n",
    "    model_parser.add_argument('--nv', type=int, default=4)\n",
    "    model_parser.add_argument('--nh', type=int, default=16)\n",
    "    model_parser.add_argument('--drop', type=float, default=0.5)\n",
    "    model_parser.add_argument('--ac_conv', type=str, default='relu')\n",
    "    model_parser.add_argument('--ac_fc', type=str, default='relu')\n",
    "\n",
    "    model_config = model_parser.parse_args(\"\")\n",
    "    model_config.L = config.L\n",
    "    \n",
    "    model = Recommender(n_iter=n_iter,\n",
    "                        batch_size=config.batch_size,\n",
    "                        learning_rate=config.learning_rate,\n",
    "                        l2=config.l2,\n",
    "                        neg_samples=config.neg_samples,\n",
    "                        model_args=model_config,\n",
    "                        use_cuda=config.use_cuda)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on https://github.com/graytowne/caser_pytorch/blob/master/train_caser.py\n",
    "\n",
    "def read_dataset(path):\n",
    "    train_path = path + '/train.txt'\n",
    "    test_path  = path + '/test.txt'\n",
    "    \n",
    "    # load dataset\n",
    "    train = Interactions(train_path)\n",
    "    train.tocsr()\n",
    "    \n",
    "    # transform triplets to sequence representation\n",
    "    train.to_sequence(5, 3)\n",
    "\n",
    "    test = Interactions(test_path,\n",
    "                        user_map=train.user_map,\n",
    "                        item_map=train.item_map)\n",
    "    test.tocsr()\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training instances: 945251\n",
      "Epoch 1 [34.0 s]\tloss=0.8104 [0.0 s]\n",
      "hits@10: 0.6478476821192053, ndcg@10: 0.3796063716863184\n",
      "Epoch 2 [31.1 s]\tloss=0.6064 [0.0 s]\n",
      "Epoch 3 [31.2 s]\tloss=0.5130 [0.0 s]\n",
      "Epoch 4 [31.5 s]\tloss=0.4586 [0.0 s]\n",
      "Epoch 5 [31.4 s]\tloss=0.4236 [0.0 s]\n",
      "Epoch 6 [31.3 s]\tloss=0.3993 [0.0 s]\n",
      "Epoch 7 [31.5 s]\tloss=0.3798 [0.0 s]\n",
      "Epoch 8 [31.6 s]\tloss=0.3644 [0.0 s]\n",
      "Epoch 9 [31.7 s]\tloss=0.3498 [0.0 s]\n",
      "Epoch 10 [31.2 s]\tloss=0.3375 [0.0 s]\n",
      "hits@10: 0.7827814569536424, ndcg@10: 0.5416348625836132\n",
      "Epoch 11 [31.4 s]\tloss=0.3274 [0.0 s]\n",
      "Epoch 12 [31.5 s]\tloss=0.3179 [0.0 s]\n",
      "Epoch 13 [31.5 s]\tloss=0.3112 [0.0 s]\n",
      "Epoch 14 [31.5 s]\tloss=0.3038 [0.0 s]\n",
      "Epoch 15 [31.3 s]\tloss=0.2987 [0.0 s]\n",
      "Epoch 16 [31.4 s]\tloss=0.2943 [0.0 s]\n",
      "Epoch 17 [31.5 s]\tloss=0.2899 [0.0 s]\n",
      "Epoch 18 [31.3 s]\tloss=0.2862 [0.0 s]\n",
      "Epoch 19 [31.0 s]\tloss=0.2834 [0.0 s]\n",
      "Epoch 20 [31.5 s]\tloss=0.2807 [0.0 s]\n",
      "hits@10: 0.7862582781456954, ndcg@10: 0.5518995905411003\n"
     ]
    }
   ],
   "source": [
    "train, test = read_dataset('data/ml-1m')\n",
    "\n",
    "model_movielens = get_model()\n",
    "model_movielens.fit(train, test, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.5 s, sys: 51.9 ms, total: 16.6 s\n",
      "Wall time: 16.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7827814569536424, 0.5473805649720678)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "evaluate_hits_ndcg(model_movielens, train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training instances: 101343\n",
      "Epoch 1 [20.5 s]\tloss=1.1125 [0.0 s]\n",
      "hits@10: 0.43677814602792003, ndcg@10: 0.2514170970868496\n",
      "Epoch 2 [3.4 s]\tloss=0.9647 [0.0 s]\n",
      "Epoch 3 [3.3 s]\tloss=0.8753 [0.0 s]\n",
      "Epoch 4 [3.3 s]\tloss=0.7835 [0.0 s]\n",
      "Epoch 5 [3.3 s]\tloss=0.6947 [0.0 s]\n",
      "Epoch 6 [20.5 s]\tloss=0.6738 [0.0 s]\n",
      "Epoch 7 [3.4 s]\tloss=0.5945 [0.0 s]\n",
      "Epoch 8 [3.3 s]\tloss=0.5345 [0.0 s]\n",
      "Epoch 9 [3.3 s]\tloss=0.4863 [0.0 s]\n",
      "Epoch 10 [3.6 s]\tloss=0.4463 [0.0 s]\n",
      "hits@10: 0.5351079859367153, ndcg@10: 0.3192788925597358\n",
      "Epoch 11 [20.7 s]\tloss=0.4504 [0.0 s]\n",
      "Epoch 12 [3.4 s]\tloss=0.4073 [0.0 s]\n",
      "Epoch 13 [3.4 s]\tloss=0.3730 [0.0 s]\n",
      "Epoch 14 [3.4 s]\tloss=0.3483 [0.0 s]\n",
      "Epoch 15 [3.4 s]\tloss=0.3228 [0.0 s]\n",
      "Epoch 16 [20.7 s]\tloss=0.3335 [0.0 s]\n",
      "Epoch 17 [3.4 s]\tloss=0.3052 [0.0 s]\n",
      "Epoch 18 [3.3 s]\tloss=0.2833 [0.0 s]\n",
      "Epoch 19 [3.3 s]\tloss=0.2644 [0.0 s]\n",
      "Epoch 20 [3.4 s]\tloss=0.2479 [0.0 s]\n",
      "hits@10: 0.5670578794262213, ndcg@10: 0.35987679579369725\n"
     ]
    }
   ],
   "source": [
    "train, test = read_dataset('data/Video')\n",
    "\n",
    "model_video = get_model()\n",
    "model_video.fit(train, test, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.4 s, sys: 60 ms, total: 26.4 s\n",
      "Wall time: 26.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5762286860581746, 0.36362269353903226)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "evaluate_hits_ndcg(model_video, train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beauty "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training instances: 121810\n",
      "Epoch 1 [61.4 s]\tloss=1.1099 [0.0 s]\n",
      "hits@10: 0.36121712222795255, ndcg@10: 0.20410765979649703\n",
      "Epoch 2 [4.1 s]\tloss=0.9602 [0.0 s]\n",
      "Epoch 3 [4.1 s]\tloss=0.8678 [0.0 s]\n",
      "Epoch 4 [4.0 s]\tloss=0.7771 [0.0 s]\n",
      "Epoch 5 [4.2 s]\tloss=0.7051 [0.0 s]\n",
      "Epoch 6 [4.3 s]\tloss=0.6401 [0.0 s]\n",
      "Epoch 7 [4.2 s]\tloss=0.5844 [0.0 s]\n",
      "Epoch 8 [4.2 s]\tloss=0.5388 [0.0 s]\n",
      "Epoch 9 [4.1 s]\tloss=0.4997 [0.0 s]\n",
      "Epoch 10 [4.1 s]\tloss=0.4680 [0.0 s]\n",
      "hits@10: 0.37818257089629476, ndcg@10: 0.22407027085130296\n",
      "Epoch 11 [4.3 s]\tloss=0.4390 [0.0 s]\n",
      "Epoch 12 [4.2 s]\tloss=0.4115 [0.0 s]\n",
      "Epoch 13 [4.1 s]\tloss=0.3878 [0.0 s]\n",
      "Epoch 14 [4.2 s]\tloss=0.3670 [0.0 s]\n",
      "Epoch 15 [4.2 s]\tloss=0.3478 [0.0 s]\n",
      "Epoch 16 [4.1 s]\tloss=0.3306 [0.0 s]\n",
      "Epoch 17 [4.1 s]\tloss=0.3165 [0.0 s]\n",
      "Epoch 18 [4.1 s]\tloss=0.3024 [0.0 s]\n",
      "Epoch 19 [4.1 s]\tloss=0.2894 [0.0 s]\n",
      "Epoch 20 [4.0 s]\tloss=0.2787 [0.0 s]\n",
      "hits@10: 0.3726298433635614, ndcg@10: 0.22878628717752122\n"
     ]
    }
   ],
   "source": [
    "train, test = read_dataset('data/Beauty')\n",
    "\n",
    "model_beauty = get_model()\n",
    "model_beauty.fit(train, test, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3642000826787929, 0.22389935923056878)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = read_dataset('data/Beauty')\n",
    "evaluate_hits_ndcg(model_beauty, train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = read_dataset('data/Steam')\n",
    "\n",
    "model_steam = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training instances: 1663064\n",
      "Epoch 1 [186.5 s]\tloss=0.7045 [0.0 s]\n",
      "hits@10: 0.7458019105077929, ndcg@10: 0.4849447217200322\n",
      "Epoch 2 [57.5 s]\tloss=0.5799 [0.0 s]\n",
      "Epoch 3 [57.6 s]\tloss=0.5117 [0.0 s]\n",
      "Epoch 4 [57.7 s]\tloss=0.4623 [0.0 s]\n",
      "Epoch 5 [58.0 s]\tloss=0.4245 [0.0 s]\n",
      "Epoch 6 [57.4 s]\tloss=0.3952 [0.0 s]\n",
      "Epoch 7 [57.3 s]\tloss=0.3716 [0.0 s]\n",
      "Epoch 8 [57.6 s]\tloss=0.3538 [0.0 s]\n",
      "Epoch 9 [57.5 s]\tloss=0.3390 [0.0 s]\n",
      "Epoch 10 [58.5 s]\tloss=0.3273 [0.0 s]\n",
      "hits@10: 0.7864175205947358, ndcg@10: 0.5319933167877577\n",
      "Epoch 11 [57.5 s]\tloss=0.3179 [0.0 s]\n",
      "Epoch 12 [57.6 s]\tloss=0.3105 [0.0 s]\n",
      "Epoch 13 [57.7 s]\tloss=0.3044 [0.0 s]\n",
      "Epoch 14 [58.3 s]\tloss=0.2991 [0.0 s]\n",
      "Epoch 15 [57.8 s]\tloss=0.2955 [0.0 s]\n",
      "Epoch 16 [57.7 s]\tloss=0.2915 [0.0 s]\n",
      "Epoch 17 [57.7 s]\tloss=0.2883 [0.0 s]\n",
      "Epoch 18 [57.5 s]\tloss=0.2861 [0.0 s]\n",
      "Epoch 19 [58.0 s]\tloss=0.2837 [0.0 s]\n",
      "Epoch 20 [58.0 s]\tloss=0.2815 [0.0 s]\n",
      "hits@10: 0.8017302082285485, ndcg@10: 0.545679037760272\n",
      "CPU times: user 22min 19s, sys: 22 s, total: 22min 41s\n",
      "Wall time: 22min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_steam.fit(train, test, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8060362173038229, 0.5476987024858936)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_hits_ndcg(model_steam, train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = read_dataset('data/Beer')\n",
    "\n",
    "model_beer = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training instances: 2859902\n",
      "Epoch 1 [141.5 s]\tloss=0.3623 [0.0 s]\n",
      "hits@10: 0.6108762210234728, ndcg@10: 0.3581718029171236\n",
      "Epoch 2 [95.1 s]\tloss=0.1348 [0.0 s]\n",
      "Epoch 3 [95.7 s]\tloss=0.1025 [0.0 s]\n",
      "Epoch 4 [94.8 s]\tloss=0.0935 [0.0 s]\n",
      "Epoch 5 [94.8 s]\tloss=0.0905 [0.0 s]\n",
      "Epoch 6 [95.7 s]\tloss=0.0891 [0.0 s]\n",
      "Epoch 7 [95.2 s]\tloss=0.0879 [0.0 s]\n",
      "Epoch 8 [94.6 s]\tloss=0.0874 [0.0 s]\n",
      "Epoch 9 [95.8 s]\tloss=0.0869 [0.0 s]\n",
      "Epoch 10 [95.2 s]\tloss=0.0864 [0.0 s]\n",
      "hits@10: 0.6952479338842975, ndcg@10: 0.4628991585782418\n",
      "Epoch 11 [94.8 s]\tloss=0.0857 [0.0 s]\n",
      "Epoch 12 [96.0 s]\tloss=0.0853 [0.0 s]\n",
      "Epoch 13 [95.2 s]\tloss=0.0849 [0.0 s]\n",
      "Epoch 14 [95.0 s]\tloss=0.0849 [0.0 s]\n",
      "Epoch 15 [95.7 s]\tloss=0.0840 [0.0 s]\n",
      "Epoch 16 [95.0 s]\tloss=0.0840 [0.0 s]\n",
      "Epoch 17 [96.1 s]\tloss=0.0836 [0.0 s]\n",
      "Epoch 18 [95.8 s]\tloss=0.0834 [0.0 s]\n",
      "Epoch 19 [95.1 s]\tloss=0.0831 [0.0 s]\n",
      "Epoch 20 [96.9 s]\tloss=0.0829 [0.0 s]\n",
      "hits@10: 0.7101789381050161, ndcg@10: 0.476839893008812\n",
      "CPU times: user 32min 52s, sys: 32.3 s, total: 33min 25s\n",
      "Wall time: 33min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_beer.fit(train, test, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
